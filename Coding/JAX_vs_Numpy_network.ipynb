{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UGrbvpD_E4m7"
      ],
      "machine_shape": "hm",
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM4Zmu6aKlCxzooCpog1Vmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarrisonSantiago/WebsiteNotebooks/blob/main/Coding/JAX_vs_Numpy_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is to show the speed difference in using jax and numpy in creating and training a neural network from scratch. It accompanies my post at https://harrisonsantiago.com/?page_id=86"
      ],
      "metadata": {
        "id": "kvPGJoC_xfgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utils"
      ],
      "metadata": {
        "id": "bLA1jK719e2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, device_put, random\n",
        "from functools import partial\n",
        "from jax import random\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Callable, Optional, Union, Tuple, List, Dict\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "def plot_results(X_test, y_test, predictions, save_path='prediction_results.png'):\n",
        "    \"\"\"Plot comprehensive results for neural network classification.\n",
        "\n",
        "    Args:\n",
        "        X_test: Test input data of shape (n_samples, 2)\n",
        "        y_test: Test target data of shape (n_samples, 3) - one-hot encoded\n",
        "        network: Trained neural network\n",
        "        save_path: Path to save the plot\n",
        "    \"\"\"\n",
        "    sns.set_theme()\n",
        "    fig = plt.figure(figsize=(20, 5))\n",
        "\n",
        "    true_classes = np.argmax(y_test, axis=1)\n",
        "    pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Create meshgrid for decision boundary\n",
        "    x_min, x_max = X_test[:, 0].min() - 0.5, X_test[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_test[:, 1].min() - 0.5, X_test[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                        np.arange(y_min, y_max, 0.02))\n",
        "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "    # Function to create smooth decision boundaries\n",
        "    def get_smooth_decision_boundary(X, y, grid_points):\n",
        "        knn = KNeighborsClassifier(n_neighbors=5)\n",
        "        knn.fit(X, y)\n",
        "        Z = knn.predict(grid_points)\n",
        "        Z = Z.reshape(xx.shape)\n",
        "        Z_smooth = gaussian_filter(Z, sigma=2)\n",
        "        return Z_smooth\n",
        "\n",
        "    # Get decision boundaries for true and predicted classes\n",
        "    Z_true = get_smooth_decision_boundary(X_test, true_classes, grid_points)\n",
        "    Z_pred = get_smooth_decision_boundary(X_test, pred_classes, grid_points)\n",
        "\n",
        "    # Plot true classes with true decision boundary\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    contour = ax1.contourf(xx, yy, Z_true, alpha=0.3, cmap='viridis', vmin=0, vmax=2)\n",
        "    scatter = ax1.scatter(X_test[:, 0], X_test[:, 1],\n",
        "                         c=true_classes,\n",
        "                         cmap='viridis',\n",
        "                         alpha=1,\n",
        "                         vmin=0,\n",
        "                         vmax=2,\n",
        "                         edgecolors='black',\n",
        "                         linewidth=1)\n",
        "    plt.colorbar(scatter, ax=ax1, ticks=[0, 1, 2])\n",
        "    ax1.set_title('True Classes with Ideal Decision Boundary')\n",
        "    ax1.set_xlabel('X1')\n",
        "    ax1.set_ylabel('X2')\n",
        "\n",
        "    # Plot predicted classes with model's decision boundary\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    contour = ax2.contourf(xx, yy, Z_pred, alpha=0.3, cmap='viridis', vmin=0, vmax=2)\n",
        "    scatter = ax2.scatter(X_test[:, 0], X_test[:, 1],\n",
        "                         c=pred_classes,\n",
        "                         cmap='viridis',\n",
        "                         alpha=1,\n",
        "                         vmin=0,\n",
        "                         vmax=2,\n",
        "                         edgecolors='black',\n",
        "                         linewidth=1)\n",
        "    plt.colorbar(scatter, ax=ax2, ticks=[0, 1, 2])\n",
        "    ax2.set_title('Predicted Classes with Model Decision Boundary')\n",
        "    ax2.set_xlabel('X1')\n",
        "    ax2.set_ylabel('X2')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    confusion = np.zeros((3, 3))\n",
        "    for t, p in zip(true_classes, pred_classes):\n",
        "        confusion[t, p] += 1\n",
        "    confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n",
        "\n",
        "    im = ax3.imshow(confusion_normalized, cmap='Blues', vmin=0, vmax=1)\n",
        "    plt.colorbar(im, ax=ax3)\n",
        "    ax3.set_title('Confusion Matrix (Normalized)')\n",
        "    ax3.set_xlabel('Predicted Class')\n",
        "    ax3.set_ylabel('True Class')\n",
        "\n",
        "    # Add text annotations to confusion matrix\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            text_color = 'white' if confusion_normalized[i, j] > 0.5 else 'black'\n",
        "            ax3.text(j, i, f'{confusion[i, j]:.0f}\\n({confusion_normalized[i, j]:.2f})',\n",
        "                    ha='center', va='center',\n",
        "                    color=text_color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_circle_data_np(n_points=100, n_classes=3, noise=0.1, max_radius=1.0):\n",
        "    \"\"\"\n",
        "    Generate concentric circles dataset for classification.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    n_points : int\n",
        "        Number of points per class\n",
        "    n_classes : int\n",
        "        Number of classes (circles)\n",
        "    noise : float\n",
        "        Standard deviation of Gaussian noise\n",
        "    max_radius : float\n",
        "        Radius of the outermost circle\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X : ndarray of shape (n_points * n_classes, 2)\n",
        "    y : ndarray of shape (n_points * n_classes,)\n",
        "    \"\"\"\n",
        "    X = np.zeros((n_points * n_classes, 2))\n",
        "    y = np.zeros(n_points * n_classes, dtype='uint8')\n",
        "\n",
        "    for class_idx in range(n_classes):\n",
        "      ix = range(n_points * class_idx, n_points * (class_idx + 1))\n",
        "\n",
        "      # Quadratically increasing spaces (more space between outer rings)\n",
        "      r = max_radius * ((n_classes - class_idx) / n_classes) ** 2\n",
        "      theta = np.linspace(0, 2*np.pi, n_points) + np.random.randn(n_points) * noise\n",
        "      r_noise = r + np.random.randn(n_points) * noise * r\n",
        "\n",
        "      # polar -> Cartesian\n",
        "      X[ix] = np.column_stack((\n",
        "          r_noise * np.cos(theta),\n",
        "          r_noise * np.sin(theta)\n",
        "      ))\n",
        "\n",
        "      y[ix] = class_idx\n",
        "\n",
        "      y_onehot = np.eye(3)[y]\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "def generate_circle_data_jax(key, n_points=100, n_classes=3, noise=0.1, max_radius=1.0):\n",
        "    \"\"\"\n",
        "    Generate concentric circles dataset for classification using JAX.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    key : jax.random.PRNGKey\n",
        "        Random number generator key\n",
        "    n_points : int\n",
        "        Number of points per class\n",
        "    n_classes : int\n",
        "        Number of classes (circles)\n",
        "    noise : float\n",
        "        Standard deviation of Gaussian noise\n",
        "    max_radius : float\n",
        "        Radius of the outermost circle\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X : ndarray of shape (n_points * n_classes, 2)\n",
        "    y : ndarray of shape (n_points * n_classes,)\n",
        "    \"\"\"\n",
        "    X = jnp.zeros((n_points * n_classes, 2))\n",
        "    y = jnp.zeros(n_points * n_classes, dtype='int32')\n",
        "\n",
        "    for class_idx in range(n_classes):\n",
        "        # Get new random key for this iteration\n",
        "        key, subkey = random.split(key)\n",
        "\n",
        "        # Convert range to array for indexing\n",
        "        ix = jnp.arange(n_points * class_idx, n_points * (class_idx + 1))\n",
        "\n",
        "        r = max_radius * ((n_classes - class_idx) / n_classes) ** 2\n",
        "        theta = jnp.linspace(0, 2*jnp.pi, n_points) + \\\n",
        "                random.normal(key, (n_points,)) * noise\n",
        "        key, subkey = random.split(key)\n",
        "        r_noise = r + random.normal(subkey, (n_points,)) * noise * r\n",
        "\n",
        "        # Convert polar coordinates to Cartesian\n",
        "        X = X.at[ix].set(jnp.stack([\n",
        "            r_noise * jnp.cos(theta),  # x coordinates\n",
        "            r_noise * jnp.sin(theta)   # y coordinates\n",
        "        ], axis=1))\n",
        "\n",
        "        y = y.at[ix].set(class_idx)\n",
        "\n",
        "    y_onehot = jnp.eye(3)[y]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y_onehot, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "rw4g8-w59gdj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Numpy Version"
      ],
      "metadata": {
        "id": "UGrbvpD_E4m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P5kzjMVrxba-"
      },
      "outputs": [],
      "source": [
        "class Layer(ABC):\n",
        "  \"\"\"Abstract base class for neural network layers.\"\"\"\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  @abstractmethod\n",
        "  def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Forward pass computation.\n",
        "\n",
        "    Args:\n",
        "        input_data: Input tensor of shape (batch_size, input_features)\n",
        "\n",
        "    Returns:\n",
        "        Output tensor of shape (batch_size, output_features)\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @abstractmethod\n",
        "  def backward(self, output_error: np.ndarray, learning_rate: float) -> np.ndarray:\n",
        "    \"\"\"Backward pass computation.\n",
        "\n",
        "    Args:\n",
        "        output_error: Gradient of the loss with respect to layer output\n",
        "        learning_rate: Learning rate for parameter updates\n",
        "\n",
        "    Returns:\n",
        "        Gradient of the loss with respect to layer input\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class DenseLayer(Layer):\n",
        "  \"\"\"Fully connected neural network layer.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size: int, output_size: int) -> None:\n",
        "    \"\"\"Initialize layer parameters.\n",
        "\n",
        "    Args:\n",
        "        input_size: Number of input features\n",
        "        output_size: Number of output features\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    limit = np.sqrt(6 / (input_size + output_size))\n",
        "    self.weights = np.random.uniform(-limit, limit, (input_size, output_size))\n",
        "    self.bias = np.zeros((1, output_size))\n",
        "\n",
        "    # Parameter gradients\n",
        "    self.weights_grad: Optional[np.ndarray] = None\n",
        "    self.bias_grad: Optional[np.ndarray] = None\n",
        "\n",
        "  def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute forward pass.\n",
        "\n",
        "    Args:\n",
        "        input_data: Input tensor of shape (batch_size, input_features)\n",
        "\n",
        "    Returns:\n",
        "        Output tensor of shape (batch_size, output_features)\n",
        "    \"\"\"\n",
        "    self.input = input_data\n",
        "    self.output = np.dot(input_data, self.weights) + self.bias\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, output_error: np.ndarray, learning_rate: float) -> np.ndarray:\n",
        "    \"\"\"Compute backward pass and update parameters.\n",
        "\n",
        "    Args:\n",
        "        output_error: Gradient of loss with respect to layer output\n",
        "        learning_rate: Learning rate for parameter updates\n",
        "\n",
        "    Returns:\n",
        "        Gradient of loss with respect to layer input\n",
        "    \"\"\"\n",
        "    # Compute gradients\n",
        "    input_error = np.dot(output_error, self.weights.T)\n",
        "    self.weights_grad = np.dot(self.input.T, output_error) / self.input.shape[0]  # Added batch normalization\n",
        "    self.bias_grad = np.sum(output_error, axis=0, keepdims=True) / self.input.shape[0]  # Added batch normalization\n",
        "\n",
        "    # Update parameters\n",
        "    self.weights -= learning_rate * self.weights_grad\n",
        "    self.bias -= learning_rate * self.bias_grad\n",
        "\n",
        "    return input_error\n",
        "\n",
        "\n",
        "class ActivationLayer(Layer):\n",
        "  \"\"\"Neural network activation layer.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "                activation_fn: Callable[[np.ndarray], np.ndarray],\n",
        "                activation_prime: Callable[[np.ndarray], np.ndarray]) -> None:\n",
        "    \"\"\"Initialize activation functions.\n",
        "\n",
        "    Args:\n",
        "        activation_fn: Activation function\n",
        "        activation_prime: Derivative of activation function\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.activation_fn = activation_fn\n",
        "    self.activation_prime = activation_prime\n",
        "\n",
        "  def forward(self, input_data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Apply activation function.\n",
        "\n",
        "    Args:\n",
        "        input_data: Input tensor\n",
        "\n",
        "    Returns:\n",
        "        Activated tensor\n",
        "    \"\"\"\n",
        "    self.input = input_data\n",
        "    self.output = self.activation_fn(self.input)\n",
        "    return self.output\n",
        "\n",
        "  def backward(self, output_error: np.ndarray, learning_rate: float) -> np.ndarray:\n",
        "    \"\"\"Compute gradient through activation function.\n",
        "\n",
        "    Args:\n",
        "        output_error: Gradient of loss with respect to layer output\n",
        "        learning_rate: Unused, kept for API consistency\n",
        "\n",
        "    Returns:\n",
        "        Gradient of loss with respect to layer input\n",
        "    \"\"\"\n",
        "    return self.activation_prime(self.input) * output_error\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LossFunctions:\n",
        "  \"\"\"Collection of loss functions and their derivatives.\"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def cross_entropy(y_true: np.ndarray,\n",
        "                    y_pred: np.ndarray,\n",
        "                    epsilon: float = 1e-15) -> float:\n",
        "    \"\"\"Categorical cross-entropy loss for multi-class classification.\n",
        "\n",
        "    Args:\n",
        "        y_true: One-hot encoded ground truth values\n",
        "        y_pred: Predicted probabilities for each class\n",
        "        epsilon: Small constant to avoid log(0)\n",
        "\n",
        "    Returns:\n",
        "        Categorical cross-entropy loss value\n",
        "    \"\"\"\n",
        "    # Clip predictions to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Calculate categorical cross-entropy\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "  @staticmethod\n",
        "  def cross_entropy_prime(y_true: np.ndarray,\n",
        "                          y_pred: np.ndarray,\n",
        "                          epsilon: float = 1e-15) -> np.ndarray:\n",
        "    \"\"\"Derivative of categorical cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "        y_true: One-hot encoded ground truth values\n",
        "        y_pred: Predicted probabilities for each class\n",
        "        epsilon: Small constant to avoid division by zero\n",
        "\n",
        "    Returns:\n",
        "        Gradient of categorical cross-entropy with respect to predictions\n",
        "    \"\"\"\n",
        "     # Clip predictions to avoid division by zero\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    return y_pred - y_true\n",
        "\n",
        "\n",
        "class Activations:\n",
        "  \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "  @staticmethod\n",
        "  def leaky_relu_prime(x: np.ndarray, alpha: float = 0.01) -> np.ndarray:\n",
        "    return np.where(x > 0, 1.0, alpha)\n",
        "\n"
      ],
      "metadata": {
        "id": "zGuph7SsS9LG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class NetworkConfig:\n",
        "    \"\"\"Neural network configuration parameters.\"\"\"\n",
        "    learning_rate: float = 0.01\n",
        "    epochs: int = 1000\n",
        "    batch_size: int = 32\n",
        "    clip_value: float = 5.0  # Add gradient clipping\n",
        "    epsilon: float = 1e-15   # Small constant to prevent division by zero\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"Simple neural network implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config: NetworkConfig = NetworkConfig()) -> None:\n",
        "        self.layers: list[Layer] = []\n",
        "        self.loss = LossFunctions.cross_entropy\n",
        "        self.loss_prime = LossFunctions.cross_entropy_prime\n",
        "        self.config = config\n",
        "\n",
        "    def add(self, layer: Layer) -> None:\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, input_data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Generate predictions with NaN checking.\"\"\"\n",
        "        output = input_data\n",
        "\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            output = layer.forward(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _clip_gradients(self, grad: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Clip gradients to prevent explosion.\"\"\"\n",
        "        return np.clip(grad, -self.config.clip_value, self.config.clip_value)\n",
        "\n",
        "    def fit(self, x_train: np.ndarray, y_train: np.ndarray) -> list[float]:\n",
        "        \"\"\"Train the neural network with NaN prevention.\"\"\"\n",
        "        samples = len(x_train)\n",
        "\n",
        "        for epoch in range(self.config.epochs):\n",
        "            epoch_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "            # Mini-batch gradient descent\n",
        "            for i in range(0, samples, self.config.batch_size):\n",
        "                batch_x = x_train[i:i + self.config.batch_size]\n",
        "                batch_y = y_train[i:i + self.config.batch_size]\n",
        "                actual_batch_size = len(batch_x)\n",
        "\n",
        "                # Forward propagation\n",
        "                output = self.predict(batch_x)\n",
        "                # Prevent division by zero in log operations\n",
        "                output = np.clip(output, self.config.epsilon, 1 - self.config.epsilon)\n",
        "\n",
        "\n",
        "                # Backward propagation\n",
        "                error = self.loss_prime(batch_y, output)\n",
        "                error = self._clip_gradients(error)  # Clip initial gradients\n",
        "\n",
        "                for j, layer in enumerate(reversed(self.layers)):\n",
        "                    error = layer.backward(error, self.config.learning_rate)\n",
        "                    error = self._clip_gradients(error)  # Clip initial gradients\n",
        ""
      ],
      "metadata": {
        "id": "MU-9l_Kxyve9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Version"
      ],
      "metadata": {
        "id": "eSkaigBDa_dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The jax version is simple, the difficulty lies in keeping a functional paradigm. I find this difficult after years of OOP."
      ],
      "metadata": {
        "id": "lgfyYQGukPsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer_params(input_size: int,\n",
        "                     output_size: int,\n",
        "                     key: random.PRNGKey) -> Dict:\n",
        "  \"\"\"\n",
        "  Return the weights and biases for a dense layer\n",
        "\n",
        "  Args:\n",
        "    - input_size: size of this layer\n",
        "    - output_size: size of the next layer\n",
        "    - key: random key for this layer\n",
        "\n",
        "  Returns:\n",
        "    - params: a dictionary of weights and biases for this layer\n",
        "  \"\"\"\n",
        "  limit = jnp.sqrt(6 / (input_size + output_size))\n",
        "  W_key, b_key = random.split(key)\n",
        "  return {\n",
        "    'weights': random.uniform(W_key, (output_size, input_size), minval=-limit, maxval=limit),\n",
        "    'bias': random.uniform(b_key, (output_size))\n",
        "    }\n",
        "\n",
        "\n",
        "@jit\n",
        "def step(params: List[Dict],\n",
        "         x: jnp.ndarray,\n",
        "         y: jnp.ndarray,\n",
        "         lr: float = 0.05):\n",
        "\n",
        "  \"\"\"Optimized training step with static learning rate\"\"\"\n",
        "\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  updated_params = []\n",
        "\n",
        "  for param, grad_param in zip(params, grads):\n",
        "    updated_param = {\n",
        "      'weights': param['weights'] - lr * grad_param['weights'],\n",
        "      'bias': param['bias'] - lr * grad_param['bias']\n",
        "    }\n",
        "    updated_params.append(updated_param)\n",
        "  return updated_params\n",
        "\n",
        "@jit\n",
        "def loss(params: List[Dict],\n",
        "         x: jnp.ndarray,\n",
        "         targets: jnp.ndarray) -> float:\n",
        "  \"\"\"Compute cross entropy loss\"\"\"\n",
        "\n",
        "  predictions = batched_predict(params, x)\n",
        "  log_softmax = predictions - jax.scipy.special.logsumexp(predictions, axis=1, keepdims=True)\n",
        "  return -jnp.mean(jnp.sum(targets * log_softmax, axis=1))\n",
        "\n",
        "@jit\n",
        "def predict(params: List[Dict], x: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Implicitly defines densley connected network\"\"\"\n",
        "  alpha = 1e-15\n",
        "  for p in params[:-1]:\n",
        "    x = jnp.dot(p['weights'], x) + p['bias']\n",
        "    x = jnp.where(x > 0, x, alpha * x)\n",
        "\n",
        "  final_weight = params[-1]['weights']\n",
        "  final_bias = params[-1]['bias']\n",
        "  return jnp.dot(final_weight, x) + final_bias\n",
        "\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        ""
      ],
      "metadata": {
        "id": "y7nKKTBAkQYM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing the Networks"
      ],
      "metadata": {
        "id": "qigqkbOM0pKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Numpy version\n",
        "\n",
        "#Get data from utils\n",
        "X_train, X_test, y_train, y_test = generate_circle_data_np(n_points= 2000)\n",
        "\n",
        "for _ in range(10):\n",
        "  # Define our network for each trial\n",
        "  network = NeuralNetwork(NetworkConfig(\n",
        "      learning_rate=0.05,\n",
        "      epochs=50,\n",
        "      batch_size=64,\n",
        "  ))\n",
        "\n",
        "  network.add(DenseLayer(2, 128))\n",
        "  network.add(ActivationLayer(Activations.leaky_relu, Activations.leaky_relu_prime))\n",
        "  network.add(DenseLayer(128, 256))\n",
        "  network.add(ActivationLayer(Activations.leaky_relu, Activations.leaky_relu_prime))\n",
        "  network.add(DenseLayer(256, 128))\n",
        "  network.add(ActivationLayer(Activations.leaky_relu, Activations.leaky_relu_prime))\n",
        "  network.add(DenseLayer(128, 3))\n",
        "\n",
        "  # Time how long training takes\n",
        "  start_time = time.time()\n",
        "  network.fit(X_train, y_train)\n",
        "  end_time = time.time()\n",
        "  print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "#plot last network for sanity check\n",
        "predictions = network.predict(X_test)\n",
        "plot_results(X_test, y_test, predictions, 'numpy_demo.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCAJwtsw_K1w",
        "outputId": "13fce429-9b79-46db-ff60-5c4411875326"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 7.30 seconds\n",
            "Training time: 7.54 seconds\n",
            "Training time: 7.76 seconds\n",
            "Training time: 7.57 seconds\n",
            "Training time: 7.71 seconds\n",
            "Training time: 7.63 seconds\n",
            "Training time: 7.61 seconds\n",
            "Training time: 7.08 seconds\n",
            "Training time: 7.55 seconds\n",
            "Training time: 7.19 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX version\n",
        "\n",
        "PRN = random.key(0)\n",
        "X_train, X_test, y_train, y_test = generate_circle_data_jax(PRN, n_points=2000)\n",
        "\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "layer_sizes = [input_dim, 128, 256, 128 ,num_classes]\n",
        "keys = random.split(PRN, len(layer_sizes))\n",
        "num_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "for _ in range(10):\n",
        "\n",
        "  #restart out network every trial\n",
        "  params = [get_layer_params(input_size, output_size, key) \\\n",
        "        for input_size, output_size, key \\\n",
        "        in zip(layer_sizes[:-1], layer_sizes[1:], keys)]\n",
        "\n",
        "  #Train the network\n",
        "  start_time = time.time()\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "      batch_x = X_train[i:i + batch_size]\n",
        "      batch_y = y_train[i:i + batch_size]\n",
        "      params = step(params, batch_x, batch_y)\n",
        "\n",
        "\n",
        "  end_time = time.time()\n",
        "  print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "#plot out last network for sanity check\n",
        "predictions = batched_predict(params, X_test)\n",
        "plot_results(X_test, y_test, predictions, 'jax_demo.png')"
      ],
      "metadata": {
        "id": "Ll-gEDGaD445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d148be-b09d-4fe0-ace5-4c701d1929d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 2.00 seconds\n",
            "Training time: 1.67 seconds\n",
            "Training time: 1.77 seconds\n",
            "Training time: 1.82 seconds\n",
            "Training time: 1.73 seconds\n",
            "Training time: 1.73 seconds\n",
            "Training time: 1.85 seconds\n",
            "Training time: 1.95 seconds\n",
            "Training time: 2.00 seconds\n",
            "Training time: 1.96 seconds\n"
          ]
        }
      ]
    }
  ]
}